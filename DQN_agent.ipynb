{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07630e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import pickle\n",
    "# import warnings\n",
    "# import copy\n",
    "# import random\n",
    "# import matplotlib.pyplot as plt\n",
    "# from collections import Counter\n",
    "# import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671fc63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipynb.fs.full.Adversarial_NIDS_Environemnt import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a2ff4ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "import math\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from torch.nn import init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7891b0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_ind = [i for i in range(len(df))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dffb240",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0bdfa48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## The neural network class with input size = dimension of state space (n_nodes+1), output layer size = 1 (action)\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    \n",
    "    def __init__(self, state_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_features= state_dim , out_features=256)\n",
    "        init.kaiming_normal_(self.fc1.weight, mode = 'fan_in')\n",
    "        self.fc2 = nn.Linear(in_features=256, out_features=128)\n",
    "        self.fc3 = nn.Linear(in_features=128, out_features=64)\n",
    "        self.out = nn.Linear(in_features=64, out_features= 11)\n",
    "    \n",
    "    def forward(self, t):\n",
    "        t = t.flatten(start_dim = 1)\n",
    "        t = F.relu(self.fc1(t))\n",
    "        t = F.relu(self.fc2(t))\n",
    "        t = F.relu(self.fc3(t))\n",
    "        t = self.out(t)\n",
    "        return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2f9bb74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## An experience class to store sate. action. next state, reward information for a particular experience\n",
    "Experience = namedtuple(\n",
    "    'Experience',\n",
    "    ('state', 'action', 'next_state', 'reward')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ebe2ee06",
   "metadata": {},
   "outputs": [],
   "source": [
    "## The replay memory class to store and sample experiences\n",
    "class ReplayMemory():\n",
    "    \n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity \n",
    "        self.memory = []\n",
    "#         self.win_memory = []\n",
    "        self.push_count = 0 # Counts how many experiences we have stored\n",
    "    \n",
    "    # Method to store experience in the Replay Memory\n",
    "    def push(self, experience):\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(experience)\n",
    "        else:\n",
    "            #Stores most recent experience and replaces the oldest experience\n",
    "            self.memory[self.push_count % self.capacity] = experience\n",
    "        self.push_count += 1\n",
    "        \n",
    "    # Method to sample from the replay memory for a given batch size\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    # Method to check whether the replay memory has enough experiences left to sample\n",
    "    def can_provide_sample(self, batch_size):\n",
    "        return len(self.memory) >= batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a32db524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Epsilon Greedy Strategy Class\n",
    "class EpsilonGreedyStrategy():\n",
    "    def __init__(self, start, end, decay):\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "        self.decay = decay\n",
    "        \n",
    "    # Function to get the exploration rate given the step at which the agent is in  \n",
    "    def get_exploration_rate(self, current_step):\n",
    "        return self.end + (self.start - self.end) * \\\n",
    "            math.exp(-1. * current_step * self.decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6969f79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the agent class which needs the epsilon greedy strategy and a device to initialize\n",
    "class Agent():\n",
    "    def __init__(self, strategy,device):\n",
    "        self.current_step = 0\n",
    "        self.strategy = strategy\n",
    "        self.device =  device\n",
    "    \n",
    "    # Method to select action via exploration or exploitation\n",
    "    def select_action(self, state_,state, policy_net):\n",
    "        rate = self.strategy.get_exploration_rate(self.current_step)\n",
    "        self.current_step += 1\n",
    "\n",
    "        if rate > random.random():\n",
    "            action = random_action(state_)\n",
    "            return torch.tensor([action]).to(self.device) # exploration\n",
    "\n",
    "        else:\n",
    "            # Turns the gradient tracking off since we want top use the policy network for inference not for training\n",
    "            with torch.no_grad():\n",
    "                return policy_net(state).argmax(dim=1).to(self.device) # exploitaion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f9aac5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initializing parameter values\n",
    "batch_size = 256\n",
    "gamma = 0.8\n",
    "eps_start = 1\n",
    "eps_end = 0.01\n",
    "eps_decay = 0.00002475\n",
    "target_update = 10\n",
    "memory_size = 100000\n",
    "lr = 0.001\n",
    "num_episodes = 50000\n",
    "episode_length = 30\n",
    "# episode_duration = 400000\n",
    "# high_val = 19\n",
    "# adv_startnode = random.randint(0,n_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2573ae41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "174a27aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initializing the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ab8bb36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating an instance of the epsilon greedy strategy class\n",
    "strategy = EpsilonGreedyStrategy(eps_start, eps_end, eps_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "73b46a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating an instance of the agent and the replay memory\n",
    "agent = Agent(strategy,device)\n",
    "memory = ReplayMemory(memory_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "55ad228f",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dim = 1525"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "841c75a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating an instance of the DQN class as policy network and cloning the same network as target network\n",
    "policy_net = DQN(state_dim).to(device)\n",
    "target_net = DQN(state_dim).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "5a6718fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DQN(\n",
       "  (fc1): Linear(in_features=1525, out_features=256, bias=True)\n",
       "  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (fc3): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (out): Linear(in_features=64, out_features=11, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Load state dict function is used to update weight parameters for a neural network in pytorch \n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "## Changes the mode of the target network to evaluation\n",
    "target_net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ca39b0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(params=policy_net.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "831c3e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Changes the data stored in experience class to tensor\n",
    "\n",
    "def extract_tensors(experiences):\n",
    "    # Convert batch of Experiences to Experience of batches\n",
    "    batch = Experience(*zip(*experiences))\n",
    "\n",
    "    t1 = torch.cat(batch.state)\n",
    "    t2 = torch.cat(batch.action)\n",
    "    t3 = torch.cat(batch.reward)\n",
    "    t4 = torch.cat(batch.next_state)\n",
    "\n",
    "    return (t1,t2,t3,t4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "7aa21b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QValues():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_current(policy_net, states, actions):\n",
    "        return policy_net(states).gather(dim = 1, index=actions.unsqueeze(-1))\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_next(target_net, next_states):\n",
    "        final_state_locations = next_states.flatten(start_dim=1) \\\n",
    "            .max(dim=1)[0].eq(1).type(torch.bool)\n",
    "        non_final_state_locations = (final_state_locations == False)\n",
    "        non_final_states = next_states[non_final_state_locations]\n",
    "        batch_size = next_states.shape[0]\n",
    "        values = torch.zeros(batch_size).to(QValues.device)\n",
    "        values[non_final_state_locations] = target_net(non_final_states).max(dim=1)[0].detach()\n",
    "        return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc75b83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
